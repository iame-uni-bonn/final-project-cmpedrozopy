{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from geopy.distance import geodesic\n",
    "import pyreadstat\n",
    "from unidecode import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Cannot save file into a non-existent directory: '/Users/moisespedrozo/final-project-cmpedrozopy/bld/data'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     16\u001b[39m     processed_data.to_pickle(produces)\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m#apply the function to the data\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[43mtask_process_distance_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mtask_process_distance_data\u001b[39m\u001b[34m(data_path, produces)\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtask_process_distance_data\u001b[39m(\n\u001b[32m     10\u001b[39m         data_path = SRC / \u001b[33m\"\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m\"\u001b[39m / \u001b[33m\"\u001b[39m\u001b[33mMission_coordinatesok.csv\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     11\u001b[39m         produces = BLD / \u001b[33m\"\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m\"\u001b[39m / \u001b[33m\"\u001b[39m\u001b[33mdistance_to_missions.pickle\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     12\u001b[39m ):\n\u001b[32m     13\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Clean the distance data set.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     \u001b[43mprocess_distance_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproduces\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m     processed_data = pd.read_csv(produces, encoding=\u001b[33m'\u001b[39m\u001b[33mlatin1\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     16\u001b[39m     processed_data.to_pickle(produces)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/final-project-cmpedrozopy/src/project_mp/data_management/process_distance_data.py:16\u001b[39m, in \u001b[36mprocess_distance_data\u001b[39m\u001b[34m(input_file, output_file)\u001b[39m\n\u001b[32m     14\u001b[39m distance_df = pd.read_csv(input_file, encoding=\u001b[33m\"\u001b[39m\u001b[33mlatin1\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     15\u001b[39m distance_df.rename(columns={\u001b[33m\"\u001b[39m\u001b[33mcountry\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mmission_country\u001b[39m\u001b[33m\"\u001b[39m})\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[43mdistance_df\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_pickle\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m distance_df\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/project_mp/lib/python3.12/site-packages/pandas/util/_decorators.py:333\u001b[39m, in \u001b[36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) > num_allow_args:\n\u001b[32m    328\u001b[39m     warnings.warn(\n\u001b[32m    329\u001b[39m         msg.format(arguments=_format_argument_list(allow_args)),\n\u001b[32m    330\u001b[39m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[32m    331\u001b[39m         stacklevel=find_stack_level(),\n\u001b[32m    332\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/project_mp/lib/python3.12/site-packages/pandas/core/generic.py:3165\u001b[39m, in \u001b[36mNDFrame.to_pickle\u001b[39m\u001b[34m(self, path, compression, protocol, storage_options)\u001b[39m\n\u001b[32m   3115\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3116\u001b[39m \u001b[33;03mPickle (serialize) object to file.\u001b[39;00m\n\u001b[32m   3117\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   3161\u001b[39m \u001b[33;03m4    4    9\u001b[39;00m\n\u001b[32m   3162\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[32m   3163\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mio\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpickle\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m to_pickle\n\u001b[32m-> \u001b[39m\u001b[32m3165\u001b[39m \u001b[43mto_pickle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3166\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3167\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3168\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3169\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3170\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3171\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/project_mp/lib/python3.12/site-packages/pandas/io/pickle.py:103\u001b[39m, in \u001b[36mto_pickle\u001b[39m\u001b[34m(obj, filepath_or_buffer, compression, protocol, storage_options)\u001b[39m\n\u001b[32m    100\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m protocol < \u001b[32m0\u001b[39m:\n\u001b[32m    101\u001b[39m     protocol = pickle.HIGHEST_PROTOCOL\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    104\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    105\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mwb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    106\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    107\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    108\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    109\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[32m    110\u001b[39m     \u001b[38;5;66;03m# letting pickle write directly to the buffer is more memory-efficient\u001b[39;00m\n\u001b[32m    111\u001b[39m     pickle.dump(obj, handles.handle, protocol=protocol)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/project_mp/lib/python3.12/site-packages/pandas/io/common.py:749\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    747\u001b[39m \u001b[38;5;66;03m# Only for write methods\u001b[39;00m\n\u001b[32m    748\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m is_path:\n\u001b[32m--> \u001b[39m\u001b[32m749\u001b[39m     \u001b[43mcheck_parent_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    751\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m compression:\n\u001b[32m    752\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m compression != \u001b[33m\"\u001b[39m\u001b[33mzstd\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    753\u001b[39m         \u001b[38;5;66;03m# compression libraries do not like an explicit text-mode\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/project_mp/lib/python3.12/site-packages/pandas/io/common.py:616\u001b[39m, in \u001b[36mcheck_parent_directory\u001b[39m\u001b[34m(path)\u001b[39m\n\u001b[32m    614\u001b[39m parent = Path(path).parent\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parent.is_dir():\n\u001b[32m--> \u001b[39m\u001b[32m616\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[33mrf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCannot save file into a non-existent directory: \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparent\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mOSError\u001b[39m: Cannot save file into a non-existent directory: '/Users/moisespedrozo/final-project-cmpedrozopy/bld/data'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pytask\n",
    "\n",
    "from project_mp.config import BLD, SRC\n",
    "from project_mp.data_management.process_distance_data import (\n",
    "    process_distance_data,\n",
    ")\n",
    "\n",
    "def task_process_distance_data(\n",
    "        data_path = SRC / \"data\" / \"Mission_coordinatesok.csv\",\n",
    "        produces = BLD / \"data\" / \"distance_to_missions.pickle\",\n",
    "):\n",
    "    \"\"\"Clean the distance data set.\"\"\"\n",
    "    process_distance_data(data_path, produces)\n",
    "    processed_data = pd.read_csv(produces, encoding='latin1')\n",
    "    processed_data.to_pickle(produces)\n",
    "\n",
    "#apply the function to the data\n",
    "task_process_distance_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write a function to do the same that I did with the distance_df data set\n",
    "def process_distance_data(input_file, output_file):\n",
    "    \"\"\"\n",
    "    Load a CSV file, rename a column for easier merging, and save the processed data as a new CSV file.\n",
    "    \n",
    "    Args:\n",
    "        input_file (str): Path to the input CSV file.\n",
    "        output_file (str): Path to save the output CSV file.\n",
    "    \"\"\"\n",
    "    distance_df = pd.read_csv(input_file, encoding='latin1')\n",
    "    distance_df.rename(columns={'country':'mission_country'}, inplace=True)\n",
    "    distance_df.to_csv(output_file, index=False)\n",
    "\n",
    "#Apply the function to \"C:\\Users\\HP\\Desktop\\Felipe\\Moises\\Distance\\Mission_coordinatesok.csv\" and save the output to \"C:\\Users\\HP\\Desktop\\Felipe\\Moises\\Distance\\epp\\distance_to_missions_epp.csv\"\n",
    "#process_distance_data(\"C:/Users/HP/Desktop/Felipe/Moises/Distance/Mission_coordinatesok.csv\", \"C:/Users/HP/Desktop/Felipe/Moises/Distance/epp/distance_to_missions_epp.csv\")\n",
    "process_distance_data(\"/Users/moisespedrozo/final-project-cmpedrozopy/src/project_mp/data/Mission_coordinatesok.csv\", \"/Users/moisespedrozo/Bonn/epp/distance_to_missions_epp.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to process shapefiles\n",
    "def shape_to_csv(department_path):\n",
    "    # Read and reproject the shp files\n",
    "    shape = gpd.read_file(department_path)\n",
    "    shape = shape.to_crs(epsg=4326)\n",
    "    # Calculate the centroid and extract the longitude and latitude\n",
    "    shape['centroid'] = shape.centroid\n",
    "    shape['longitude'] = shape['centroid'].x\n",
    "    shape['latitude'] = shape['centroid'].y\n",
    "    # Save as CSV in the same folder as the shapefile for easier access\n",
    "    output_csv = department_path.replace(\".shp\", \".csv\")\n",
    "    shape.to_csv(output_csv, index=False)\n",
    "\n",
    "# Root directory containing the folders and shapefiles\n",
    "root_dir = \"Users/moisespedrozo/Bonn/epp/shape_files\"\n",
    "\n",
    "# Go through all subdirectories and apply the function\n",
    "for root, dirs, files in os.walk(root_dir):\n",
    "    for file in files:\n",
    "        if file.endswith(\".shp\") and file.startswith(\"Distritos_\"):\n",
    "            department_path = os.path.join(root, file) \n",
    "            shape_to_csv(department_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from project_mp.config import BLD, SRC\n",
    "from project_mp.data_management.shape_to_csv import shape_to_csv\n",
    "\n",
    "def task_process_shapefiles(\n",
    "        root_dir=SRC / \"shape_files\",\n",
    "        produces=[BLD / \"data\" / f\"{file.replace('.shp', '.csv')}\" for file in os.listdir(SRC / \"shape_files\") if file.endswith(\".shp\") and file.startswith(\"Distritos_\")]\n",
    "):\n",
    "    \"\"\"Convert shapefiles to CSV format, extracting centroid coordinates.\"\"\"\n",
    "    for file in os.listdir(root_dir):\n",
    "        if file.endswith(\".shp\") and file.startswith(\"Distritos_\"):\n",
    "            department_path = root_dir / file\n",
    "            shape_to_csv(department_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending: C:/Users/HP/Desktop/Felipe/Moises/Distance/shape_files\\guaira_shp_data.csv\n",
      "Appending: C:/Users/HP/Desktop/Felipe/Moises/Distance/shape_files\\alto_parana\\Distritos_Alto_Parana.csv\n",
      "Appending: C:/Users/HP/Desktop/Felipe/Moises/Distance/shape_files\\caazapa\\Distritos_Caazapa.csv\n",
      "Appending: C:/Users/HP/Desktop/Felipe/Moises/Distance/shape_files\\guaira\\Distritos_Guaira.csv\n",
      "Appending: C:/Users/HP/Desktop/Felipe/Moises/Distance/shape_files\\itapua\\Distritos_Itapua.csv\n",
      "Appending: C:/Users/HP/Desktop/Felipe/Moises/Distance/shape_files\\misiones\\Distritos_Misiones.csv\n",
      "All CSV files appended and saved to: C:/Users/HP/Desktop/Felipe/Moises/Distance/epp/distritos_combined.csv\n"
     ]
    }
   ],
   "source": [
    "def append_csvs_to_one(output_file, root_dir):\n",
    "    # List to store individual DataFrames\n",
    "    all_data = []\n",
    "\n",
    "    # Walk through all subdirectories to find CSV files\n",
    "    for root, dirs, files in os.walk(root_dir):\n",
    "        for file in files:\n",
    "            if file.endswith(\".csv\"):  # Process only CSV files\n",
    "                file_path = os.path.join(root, file)\n",
    "                print(f\"Appending: {file_path}\")\n",
    "                # Read the CSV and append to the list\n",
    "                df = pd.read_csv(file_path)\n",
    "                all_data.append(df)\n",
    "\n",
    "    # Concatenate all DataFrames into one\n",
    "    deparmentos_shape_ok = pd.concat(all_data, ignore_index=True)\n",
    "    # Delete duplicates in CLAVE variable\n",
    "    deparmentos_shape_ok = deparmentos_shape_ok.drop_duplicates(subset=['CLAVE'])\n",
    "\n",
    "    # Save the combined DataFrame as a single CSV\n",
    "    deparmentos_shape_ok.to_csv(output_file, index=False)\n",
    "    print(f\"All CSV files appended and saved to: {output_file}\")\n",
    "\n",
    "#Apply the function\n",
    "root_dir = \"C:/Users/HP/Desktop/Felipe/Moises/Distance/shape_files\"\n",
    "output_file = \"C:/Users/HP/Desktop/Felipe/Moises/Distance/epp/distritos_combined.csv\"\n",
    "\n",
    "append_csvs_to_one(output_file, root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closest missions have been calculated and saved!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Function to compute the closest mission for each department\n",
    "def find_closest_location(distritos, missions):\n",
    "    # Normalize longitude and latitude, except for rows where DPTO_DESC == 'GUAIR√Å'\n",
    "    \n",
    "\n",
    "    results = []\n",
    "\n",
    "    for index, dep in distritos.iterrows():\n",
    "        # Skip if department coordinates are invalid\n",
    "        if pd.isna(dep['latitude']) or pd.isna(dep['longitude']):\n",
    "            continue\n",
    "\n",
    "        dep_coords = (dep['latitude'], dep['longitude'])\n",
    "        \n",
    "        # Calculate distances to all missions\n",
    "        missions['distance'] = missions.apply(\n",
    "            lambda x: geodesic(dep_coords, (x['latitude'], x['longitude'])).kilometers, axis=1\n",
    "        )\n",
    "        \n",
    "        # Find the closest mission and keep the DPTO_DESC from distritos\n",
    "        closest_mission = missions.loc[missions['distance'].idxmin()]\n",
    "        results.append({\n",
    "            'department_index': index,\n",
    "            'department_code': dep.get('DPTO', 'Unknown'),\n",
    "            'department_name': dep.get('DPTO_DESC', 'Unknown'),\n",
    "            'distrito': dep.get('DIST_DESC', 'Unknown'),\n",
    "            'mission_name': closest_mission['mission_name'],\n",
    "            'mission_latitude': closest_mission['latitude'],\n",
    "            'mission_longitude': closest_mission['longitude'],\n",
    "            'distance_to_mission_km': closest_mission['distance']\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Example usage\n",
    "distritos = pd.read_csv(\"C:/Users/HP/Desktop/Felipe/Moises/Distance/epp/distritos_combined.csv\")\n",
    "missions = pd.read_csv(\"C:/Users/HP/Desktop/Felipe/Moises/Distance/epp/distance_to_missions_epp.csv\")\n",
    "\n",
    "# Compute the closest missions\n",
    "closest_missions_df = find_closest_location(distritos, missions)\n",
    "\n",
    "# Save the results\n",
    "closest_missions_df.to_csv(\"C:/Users/HP/Desktop/Felipe/Moises/Distance/closest_missions.csv\", index=False)\n",
    "\n",
    "print(\"Closest missions have been calculated and saved!\")\n",
    "\n",
    "#Now, make a function so that DIST_DESC observations are in lowercase and clean the name of the cities such that \"ascii\" characters are removed\n",
    "def clean_city_names(input_file, output_file):\n",
    "    # Load the data\n",
    "    closest_missions = pd.read_csv(input_file, encoding='latin1')\n",
    "    \n",
    "    # Lowercase the DIST_DESC column\n",
    "    closest_missions['distrito'] = closest_missions['distrito'].str.lower()\n",
    "    # Remove accents and non-ASCII characters\n",
    "    closest_missions['distrito'] = closest_missions['distrito'].apply(lambda x: unidecode(x))\n",
    "    # Later, I will merge this data set with the survey data. So, I will use a key with the cleaned city name + department code\n",
    "    # Thus, If departamento has only one digit, add a leading zero. Also, make department_code string first\n",
    "    #closest_missions['department_code'] = closest_missions['department_code'].astype(str)\n",
    "    #closest_missions['department_code'] = closest_missions['department_code'].apply(lambda x: f\"0{x}\" if len(str(x)) == 1 else x)\n",
    "\n",
    "    # Save the cleaned data\n",
    "    closest_missions.to_csv(output_file, index=False, encoding='latin1')\n",
    "    # Remove non-ASCII characters from the distrito column\n",
    "   # closest_missions['distrito'] = closest_missions['distrito'].str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')\n",
    "    # Save the cleaned data\n",
    "    #closest_missions.to_csv(output_file, index=False)\n",
    "#Apply the function to \"C:/Users/HP/Desktop/Felipe/Moises/Distance/closest_missions.csv\" and save the output to \"C:/Users/HP/Desktop/Felipe/Moises/Distance/epp/closest_missions_cleaned.csv\"\n",
    "clean_city_names(\"C:/Users/HP/Desktop/Felipe/Moises/Distance/closest_missions.csv\", \"C:/Users/HP/Desktop/Felipe/Moises/Distance/epp/closest_missions_cleaned.csv\")\n",
    "\n",
    "#Now, generate a key to merge closest_missions_cleaned with survey data later.\n",
    "#The key should be the cleaned city name + department code\n",
    "#Hence, first geberate a function such that If departamento has only one digit, add a leading zero\n",
    "def add_leading_zero(x):\n",
    "    \"\"\"\n",
    "    Add a leading zero to single-digit department codes.\n",
    "    \n",
    "    Args:\n",
    "        x (int or str): The department code.\n",
    "    \n",
    "    Returns:\n",
    "        str: The department code with a leading zero if necessary.\n",
    "    \"\"\"\n",
    "    # Convert to string\n",
    "    x = str(x)\n",
    "    # Check if the length is 1\n",
    "    if len(x) == 1:\n",
    "        return f\"0{x}\"\n",
    "    return x\n",
    "\n",
    "#Now, apply the function to the department_code column of \"C:/Users/HP/Desktop/Felipe/Moises/Distance/epp/closest_missions_cleaned.csv\"\n",
    "closest_missions = pd.read_csv(\"C:/Users/HP/Desktop/Felipe/Moises/Distance/epp/closest_missions_cleaned.csv\", encoding='latin1')\n",
    "closest_missions['department_code'] = closest_missions['department_code'].apply(add_leading_zero)\n",
    "\n",
    "#Needs a function to generate the key\n",
    "#Finally, generate the key\n",
    "def generate_key(closest_missions):\n",
    "    \"\"\"\n",
    "    Generate a key for merging the closest missions data with survey data.\n",
    "    \n",
    "    Args:\n",
    "        closest_missions (pd.DataFrame): The closest missions data.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: The closest missions data with a key column.\n",
    "    \"\"\"\n",
    "    # Generate the key\n",
    "    closest_missions['key'] = closest_missions['distrito'] + closest_missions['department_code']\n",
    "    return closest_missions\n",
    "\n",
    "#Save the data\n",
    "generate_key(closest_missions).to_csv(\"C:/Users/HP/Desktop/Felipe/Moises/Distance/epp/closest_missions_cleaned.csv\", index=False, encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before leading zero fix: [10  8  4  6  7]\n",
      "After leading zero fix: ['10' '08' '04' '06' '07']\n",
      "   nro_cuestionario  tipo_formulario  departamento           distrito  \\\n",
      "0              1241                1            10       hernandarias   \n",
      "1              1265                2             8  san juan bautista   \n",
      "2              1042                1             4               Numi   \n",
      "3              1089                2             8         san miguel   \n",
      "4              1127                2             8         san miguel   \n",
      "\n",
      "   localidad          s          o  altitud  gps_nro  fecha_entrevista_dia  \\\n",
      "0        564 -25.410032 -54.645100    227.0       21                    27   \n",
      "1        550 -26.674168 -57.145817    136.0       21                    23   \n",
      "2        517 -25.954933 -56.329567    139.0       12                    18   \n",
      "3        531 -26.536833 -57.043083    124.0       21                    17   \n",
      "4        507 -26.532932 -57.042450    132.0       22                    17   \n",
      "\n",
      "   ...  cheating2  rotter1  rotter2  rotter4  rotter5  rotter9  rotter  \\\n",
      "0  ...        1.0      5.0      4.0      4.0      4.0      4.0    33.0   \n",
      "1  ...        0.0      4.0      3.0      4.0      4.0      3.0    34.0   \n",
      "2  ...        1.0      5.0      4.0      4.0      4.0      2.0    34.0   \n",
      "3  ...        1.0      5.0      4.0      5.0      5.0      1.0    41.0   \n",
      "4  ...        0.0      4.0      4.0      4.0      4.0      4.0    35.0   \n",
      "\n",
      "   treatprime male  desirability  \n",
      "0         0.0  0.0     -0.300753  \n",
      "1         0.0  0.0     -0.300753  \n",
      "2         0.0  0.0     -0.300753  \n",
      "3         0.0  0.0     -0.300753  \n",
      "4         0.0  0.0     -0.300753  \n",
      "\n",
      "[5 rows x 211 columns]\n"
     ]
    }
   ],
   "source": [
    "#Define a function to read the survey data\n",
    "def process_survey_data(input_file, output_file):\n",
    "    # Read the .dta file using pyreadstat\n",
    "    survey_data, metadata = pyreadstat.read_dta(input_file)\n",
    "    # Replace the codes in 'distrito' with their corresponding labels\n",
    "    distrito_labels = metadata.value_labels[metadata.variable_to_label['distrito']]\n",
    "    survey_data['distrito'] = survey_data['distrito'].map(distrito_labels)\n",
    "    #Make departamento variable string\n",
    "    print(\"Before leading zero fix:\", survey_data['departamento'].unique())\n",
    "    survey_data['departamento'] = survey_data['departamento'].astype(str)\n",
    "    # If departamento has only one digit, add a leading zero\n",
    "    survey_data['departamento'] = survey_data['departamento'].apply(\n",
    "    lambda x: x.zfill(2) if x.isdigit() else x\n",
    "    )\n",
    "    print(\"After leading zero fix:\", survey_data['departamento'].unique())\n",
    "    # Remove accents and non-ASCII characters\n",
    "    survey_data['distrito'] = survey_data['distrito'].apply(lambda x: unidecode(x))\n",
    "    # Save the cleaned data\n",
    "    survey_data.to_csv(output_file, index=False, encoding='latin1')\n",
    "    # Return the cleaned data\n",
    "    return survey_data\n",
    "\n",
    "#Apply the function to the survey_data\n",
    "survey_data = process_survey_data(\"C:/Users/HP/Desktop/Felipe/Encuesta/survey_final.dta\", \"C:/Users/HP/Desktop/Felipe/Moises/Distance/epp/survey_data_cleaned.csv\")\n",
    "\n",
    "#Save the data\n",
    "survey_data_cleaned = pd.read_csv(\n",
    "    \"C:/Users/HP/Desktop/Felipe/Moises/Distance/epp/survey_data_cleaned.csv\", \n",
    "    encoding='latin1'\n",
    ")\n",
    "\n",
    "print(survey_data_cleaned.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before processing 'departamento': [10  8  4  6  7]\n",
      "After processing 'departamento': ['10' '08' '04' '06' '07']\n"
     ]
    }
   ],
   "source": [
    "# Define a function to process survey data\n",
    "def process_survey_data(input_file, output_file):\n",
    "    # Read the .dta file using pyreadstat\n",
    "    survey_data, metadata = pyreadstat.read_dta(input_file)\n",
    "    \n",
    "    # Replace the codes in 'distrito' with their corresponding labels\n",
    "    distrito_labels = metadata.value_labels[metadata.variable_to_label['distrito']]\n",
    "    survey_data['distrito'] = survey_data['distrito'].map(distrito_labels)\n",
    "    \n",
    "    # Debug: Inspect 'departamento' before processing\n",
    "    print(\"Before processing 'departamento':\", survey_data['departamento'].unique())\n",
    "    \n",
    "    # Ensure 'departamento' is a string and strip whitespace\n",
    "    survey_data['departamento'] = survey_data['departamento'].astype(str).str.strip()\n",
    "    \n",
    "    # Add a leading zero if 'departamento' has only one digit\n",
    "    survey_data['departamento'] = survey_data['departamento'].apply(\n",
    "    lambda x: x.zfill(2) if x.isdigit() else x\n",
    "    )\n",
    "    \n",
    "    # Debug: Inspect 'departamento' after processing\n",
    "    print(\"After processing 'departamento':\", survey_data['departamento'].unique())\n",
    "    \n",
    "    # Remove accents and non-ASCII characters from 'distrito'\n",
    "    survey_data['distrito'] = survey_data['distrito'].apply(lambda x: unidecode(x) if isinstance(x, str) else x)\n",
    "    \n",
    "    # Save the cleaned data\n",
    "    survey_data.to_csv(output_file, index=False, encoding='latin1')\n",
    "    \n",
    "    # Return the cleaned DataFrame\n",
    "    return survey_data\n",
    "\n",
    "# Apply the function to the survey data\n",
    "survey_data = process_survey_data(\n",
    "    \"C:/Users/HP/Desktop/Felipe/Encuesta/survey_final.dta\", \n",
    "    \"C:/Users/HP/Desktop/Felipe/Moises/Distance/epp/survey_data_cleaned.csv\"\n",
    ")\n",
    "\n",
    "# Load the saved data to confirm it's cleaned\n",
    "#survey_data_cleaned = pd.read_excel(\"survey_data_cleaned.xlsx\")\n",
    "#print(survey_data_cleaned.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Define a function to generate a key for the survey data so I can merge later with the closest_missions data\n",
    "def generate_key(survey_data):\n",
    "    #Create a new column 'key' that combines 'distrito' and dptocode\n",
    "    survey_data['key'] = survey_data['distrito'] + survey_data['departamento'].astype(str)\n",
    "\n",
    "#Apply the function to the survey data\n",
    "survey_data = pd.read_csv(\"C:/Users/HP/Desktop/Felipe/Moises/Distance/epp/survey_data_cleaned.csv\", encoding='latin1')\n",
    "generate_key(survey_data)\n",
    "#Save the data\n",
    "survey_data.to_csv(\"C:/Users/HP/Desktop/Felipe/Moises/Distance/epp/survey_data_cleaned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded survey data with 560 rows.\n",
      "Loaded closest missions data with 91 rows.\n",
      "Merged data contains 560 rows.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def merge_survey_and_missions(survey_file, missions_file, key_column):\n",
    "    \"\"\"\n",
    "    Loads survey data and closest missions data, then merges them on the specified key column.\n",
    "\n",
    "    Parameters:\n",
    "    - survey_file (str): Path to the survey data CSV file.\n",
    "    - missions_file (str): Path to the closest missions data CSV file.\n",
    "    - key_column (str): Column name to merge on.\n",
    "    - encoding (str): Encoding of the CSV files (default is 'latin1').\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: Merged DataFrame.\n",
    "    \"\"\"        \n",
    "    # Load the survey data\n",
    "    survey_data = pd.read_csv(survey_file)\n",
    "    print(f\"Loaded survey data with {len(survey_data)} rows.\")\n",
    "\n",
    "    # Load the closest missions data\n",
    "    closest_missions = pd.read_csv(missions_file)\n",
    "    print(f\"Loaded closest missions data with {len(closest_missions)} rows.\")\n",
    "\n",
    "    # Merge the two data sets on the key column\n",
    "    merged_data = pd.merge(survey_data, closest_missions, on=key_column, how='left')\n",
    "    print(f\"Merged data contains {len(merged_data)} rows.\")\n",
    "    # Save the merged data\n",
    "    merged_data.to_csv(\"C:/Users/HP/Desktop/Felipe/Moises/Distance/epp/survey_with_missions.csv\", index=False)\n",
    "\n",
    "survey_file = \"C:/Users/HP/Desktop/Felipe/Moises/Distance/epp/survey_data_cleaned.csv\"\n",
    "missions_file = \"C:/Users/HP/Desktop/Felipe/Moises/Distance/epp/closest_missions_cleaned.csv\"\n",
    "#Apply the function to the survey data and closest_missions data\n",
    "merge_survey_and_missions(\"C:/Users/HP/Desktop/Felipe/Moises/Distance/epp/survey_data_cleaned.csv\", \"C:/Users/HP/Desktop/Felipe/Moises/Distance/epp/closest_missions_cleaned.csv\", \"key\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project_mp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
